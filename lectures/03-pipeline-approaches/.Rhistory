# 1. paste the columns together and tokenize for each record
shingles <- apply(dat, 1, function(x) {
# tokenize strings
tokenize_character_shingles(paste(x[-1], collapse=" "), n = 3)[[1]]
})
# 2. Jaccard similarity between pairs
jaccard <- expand.grid(record1 = seq_len(n), # empty holder for similarities
record2 = seq_len(n))
# don't need to compare the same things twice
jaccard <- jaccard[jaccard$record1 < jaccard$record2,]
time <- Sys.time() # for timing comparison
jaccard$similarity <- apply(jaccard, 1, function(pair) {
jaccard_similarity(shingles[[pair[1]]], shingles[[pair[2]]]) # get jaccard for each pair
})
time <- difftime(Sys.time(), time, units = "secs") # timing
## ----your-turn2-plot----
# plot the jaccard similarities for each pair of records
ggplot(jaccard) +
geom_raster(aes(x = record1, y = record2, fill=similarity)) +
theme(aspect.ratio = 1) +
scale_fill_gradient("Jaccard similarity") +
xlab("Record id") + ylab("Record id")
## ----hash-tokens----------------------------------------------
# instead store hash values (less memory)
hashed_shingles <- apply(dat, 1, function(x) {
string <- paste(x[-1], collapse=" ") # get the string
shingles <- tokenize_character_shingles(string, n = 3)[[1]] # 3-shingles
hash_string(shingles) # return hashed shingles
})
## ----hash-tokens-jaccard-------------------------------------
# Jaccard similarity on hashed shingles
hashed_jaccard <- expand.grid(record1 = seq_len(n), record2 = seq_len(n))
# don't need to compare the same things twice
hashed_jaccard <- hashed_jaccard[hashed_jaccard$record1 < hashed_jaccard$record2,]
time <- Sys.time() # see how long this takes
hashed_jaccard$similarity <- apply(hashed_jaccard, 1, function(pair) {
jaccard_similarity(hashed_shingles[[pair[1]]], hashed_shingles[[pair[2]]])
}) # get jaccard for each hashed pair
time <- difftime(Sys.time(), time, units = "secs") # timing
## ----characteristic------------------------------------------
# return if an item is in a list
item_in_list <- function(item, list) {
as.integer(item %in% list)
}
char_mat <- data.frame(item = unique(unlist(hashed_shingles)))
# for each hashed shingle, see if it is in each row
contained <- lapply(hashed_shingles, function(col) {
vapply(char_mat$item, FUN = item_in_list, FUN.VALUE = integer(1), list = col)
})
char_mat <- do.call(cbind, contained) # list to matrix
rownames(char_mat) <- unique(unlist(hashed_shingles)) # row names
colnames(char_mat) <- paste("Record", seq_len(nrow(dat))) # column names
# inspect results
kable(char_mat[10:15, 1:5])
## ----minhash-1-----------------------------------------------
# set seed for reproducibility
set.seed(02082018)
get_sig <- function(char_mat) {
# get permutation order
permute_order <- sample(seq_len(nrow(char_mat)))
# get min location of "1" for each column (apply(2, ...))
t(apply(char_mat[permute_order, ], 2, function(col) min(which(col == 1))))
}
library(igraph) #graph package
# think of each record as a node
# there is an edge between nodes if they are candidates
g <- make_empty_graph(n, directed = FALSE) # empty graph
## ----show-package-lsh-----------------------------
# choose appropriate num of bands
b <- 90
# create the minhash function
minhash <- minhash_generator(n = m, seed = 02082018)
set.seed(02082018)
# function to get signature for 1 permutation
get_sig <- function(char_mat) {
# get permutation order
permute_order <- sample(seq_len(nrow(char_mat)))
# get min location of "1" for each column (apply(2, ...))
t(apply(char_mat[permute_order, ], 2, function(col) min(which(col == 1))))
}
# repeat many times
m <- 360
sig_mat <- matrix(NA, nrow=m, ncol=ncol(char_mat)) #empty matrix
for(i in 1:m) {
sig_mat[i, ] <- get_sig(char_mat) #fill matrix
}
colnames(sig_mat) <- colnames(char_mat) #column names
# inspect results
kable(sig_mat[1:10, 1:5])
## ----jaccard-sig-------------------------------
# add jaccard similarity approximated from the minhash to compare
# number of agreements over the total number of combinations
hashed_jaccard$similarity_minhash <- apply(hashed_jaccard, 1, function(row) {
sum(sig_mat[, row[["record1"]]] == sig_mat[, row[["record2"]]])/nrow(sig_mat)
})
# how far off is this approximation? plot differences
qplot(hashed_jaccard$similarity_minhash - hashed_jaccard$similarity) +
xlab("Difference between Jaccard similarity and minhash approximation")
## ----banding---------------------------------------------
# view the signature matrix
print(xtable::xtable(sig_mat[1:10, 1:5]), hline.after = c(-1,0,5,10), comment=F)
## ----inclusion-probs----
# library to get divisors of m
library(numbers)
# repeat many times
m <- 360
sig_mat <- matrix(NA, nrow=m, ncol=ncol(char_mat)) #empty matrix
for(i in 1:m) {
sig_mat[i, ] <- get_sig(char_mat) #fill matrix
}
colnames(sig_mat) <- colnames(char_mat) #column names
library(knitr)
library(ggplot2)
library(RecordLinkage)
## ----your-turn1----------------------------------------------------------
# load RL data
data("RLdata500")
# select only 2 records
records <- RLdata500[129:130, c(1,3)]
names(records) <- c("First name", "Last name")
# inspect records
kable(records)
## ----helpful-packages---------------------------------------
library(textreuse) # text reuse/document similarity
library(tokenizers) # shingles
library(RLdata) # data library
data(cora) # load the cora data set
str(cora) # structure of cora
## ----your-turn2-sol-------------------------------
# get only the columns we want
n <- nrow(cora) # number of records
dat <- data.frame(id = seq_len(n)) # create id column
dat <- cbind(dat, cora[, c("title", "authors", "journal")]) # get columnds we want
shingles <- apply(dat, 1, function(x) {
# tokenize strings
tokenize_character_shingles(paste(x[-1], collapse=" "), n = 3)[[1]]
})
hashed_shingles <- apply(dat, 1, function(x) {
string <- paste(x[-1], collapse=" ") # get the string
shingles <- tokenize_character_shingles(string, n = 3)[[1]] # 3-shingles
hash_string(shingles) # return hashed shingles
})
## ----hash-tokens-jaccard-------------------------------------
# Jaccard similarity on hashed shingles
hashed_jaccard <- expand.grid(record1 = seq_len(n), record2 = seq_len(n))
# don't need to compare the same things twice
hashed_jaccard <- hashed_jaccard[hashed_jaccard$record1 < hashed_jaccard$record2,]
item_in_list <- function(item, list) {
as.integer(item %in% list)
}
# get the characteristic matrix
# items are all the unique hash values
# columns will be each record
# we want to keep track of where each hash is included
char_mat <- data.frame(item = unique(unlist(hashed_shingles)))
contained <- lapply(hashed_shingles, function(col) {
vapply(char_mat$item, FUN = item_in_list, FUN.VALUE = integer(1), list = col)
})
char_mat <- do.call(cbind, contained) # list to matrix
rownames(char_mat) <- unique(unlist(hashed_shingles)) # row names
colnames(char_mat) <- paste("Record", seq_len(nrow(dat))) # column names
# inspect results
kable(char_mat[10:15, 1:5])
## ----minhash-1-----------------------------------------------
# set seed for reproducibility
set.seed(02082018)
get_sig <- function(char_mat) {
# get permutation order
permute_order <- sample(seq_len(nrow(char_mat)))
# get min location of "1" for each column (apply(2, ...))
t(apply(char_mat[permute_order, ], 2, function(col) min(which(col == 1))))
}
# repeat many times
m <- 360
sig_mat <- matrix(NA, nrow=m, ncol=ncol(char_mat)) #empty matrix
for(i in 1:m) {
sig_mat[i, ] <- get_sig(char_mat) #fill matrix
}
colnames(sig_mat) <- colnames(char_mat) #column names
# inspect results
kable(sig_mat[1:10, 1:5])
hashed_jaccard$similarity_minhash <- apply(hashed_jaccard, 1, function(row) {
sum(sig_mat[, row[["record1"]]] == sig_mat[, row[["record2"]]])/nrow(sig_mat)
})
# look at probability of binned together for various bin sizes and similarity values
bin_probs <- expand.grid(s = c(.25, .75), h = m, b = divisors(m))
bin_probs$prob <- apply(bin_probs, 1, function(x) lsh_probability(x[["h"]], x[["b"]], x[["s"]]))
## ----show-package-lsh-----------------------------
# choose appropriate num of bands
b <- 90
# create the minhash function
minhash <- minhash_generator(n = m, seed = 02082018)
docs <- apply(dat, 1, function(x) paste(x[-1], collapse = " ")) # get strings
names(docs) <- dat$id # add id as names in vector
corpus <- TextReuseCorpus(text = docs, # dataset
tokenizer = tokenize_character_shingles, n = 3, simplify = TRUE, # shingles
progress = FALSE, # quietly
keep_tokens = TRUE, # store shingles
minhash_func = minhash) # use minhash
# perform lsh to get buckets
buckets <- lsh(corpus, bands = b, progress = FALSE)
# grab candidate pairs
candidates <- lsh_candidates(buckets)
# get Jaccard similarities only for candidates
lsh_jaccard <- lsh_compare(candidates, corpus, jaccard_similarity, progress = FALSE)
## ---- lsh-plot-----------------------------------------------------------
# plot jaccard similarities that are candidates
qplot(lsh_jaccard$score)
## ---- lsh-plot-----------------------------------------------------------
# plot jaccard similarities that are candidates
qplot(lsh_jaccard$score)
# get Jaccard similarities only for candidates
lsh_jaccard <- lsh_compare(candidates, corpus, jaccard_similarity, progress = FALSE)
## ---- lsh-plot-----------------------------------------------------------
# plot jaccard similarities that are candidates
qplot(lsh_jaccard$score)
library(igraph) #graph package
# think of each record as a node
# there is an edge between nodes if they are candidates
g <- make_empty_graph(n, directed = FALSE) # empty graph
g <- add_edges(g, as.vector(t(candidates[, 1:2]))) # candidate edges
as.vector(t(candidates[, 1:2])
)
as.vector((candidates[, 1:2])
)
g <- add_edges(g, as.vector((candidates[, 1:2]))) # candidate edges
(candidates[, 1:2])
as.vector((candidates[, 1:2]))
add_edges(g, as.vector((candidates[, 1:2])))
candidates[, 1:2])
as.vector((candidates[, 1:2]))
type(as.vector(t(candidates[, 1:2]))
)
class(as.vector(t(candidates[, 1:2]))
)
g <- add_edges(g, is.vector((candidates[, 1:2]))) # candidate edges
g <- add_edges(g, is.vector(t(candidates[, 1:2]))) # candidate edges
g <- add_edges(g, is.vector((candidates[, 1:2]))) # candidate edges
g
g <- set_vertex_attr(g, "id", value = dat$id) # add id
# get custers, these are the blocks
clust <- components(g, "weak") # get clusters
blocks <- data.frame(id = V(g)$id, # record id
block = clust$membership) # block number
head(blocks)
library("blink")
library(RecordLinkage)
data("RLdata500") # load data
head(RLdata500) # take a look
# categorical variables
X.c <- as.matrix(RLdata500[, c("by","bm","bd")])
# string variables
X.s <- as.matrix(RLdata500[, c("fname_c1", "lname_c1")])
X.c <- as.matrix(RLdata500[, c("by","bm","bd")])
# string variables
X.s <- as.matrix(RLdata500[, c("fname_c1", "lname_c1")])
# keep track of which rows of are in which files
file.num <- rep(c(1, 2, 3), c(200, 150, 150))
a <- 1
b <- 999
d <- function(s1, s2) {
adist(s1, s2) # approximate string distance
}
# steepness parameter
c <- 1
lam.gs <- rl.gibbs(file.num = file.num, # file
X.s = X.s, X.c = X.c, # data
num.gs = 100, # iterations
a = a, b = b, # prior params
c = c, d = d, # distortion
M = 500) # max # latents
install.packages(“bLink_0.1.0.tar.gz”)
install.packages("bLink_0.1.0.tar.gz")
install.packages("microclustr")
install.packages("blink")
install.packages("blink")
library("blink")
data("RLdata500") # load data
head(RLdata500) # take a look
install.packages("devtools::install_github(“cleanzr/RLdata")
install.packages("devtools::install_github("cleanzr/RLdata")
library(devtools)
install.packages("devtools::install_github("cleanzr/RLdata")
install_github("cleanzr/RLdata")
data(cora)
install_github("cleanzr/RLdata")
library(RLdata)
data("cora")
data("cora_gold")
library(blink)
library(tidyverse)
################
# PREPARE DATA #
################
df <- read.csv("sv-standard.csv")
################
# PREPARE DATA #
################
#df <- read.csv("sv-standard.csv")
#df <- read.csv("untrc.csv")
df <- read_csv("sv-mauricio.csv")
################
# PREPARE DATA #
################
#df <- read.csv("sv-standard.csv")
#df <- read.csv("untrc.csv")
df <- read.csv("sv-mauricio.csv")
#
head(df)
dim(df)
#df <- df[-c(1,2)]
ent_id <- df$HandID
df <- df[!is.na(ent_id),]
ent_id <- ent_id[!is.na(ent_id)]
df$ID <- 1:nrow(df)
numRecords <- nrow(df)
knownFirstname <- read_csv("./known-firstname.csv")
knownFirstname <- knownFirstname$token[knownFirstname$known == 1]
knownLastname <- read_csv("./known-lastname.csv")
knownLastname <- knownLastname$token[knownLastname$known == 1]
################
# PREPARE DATA #
################
#df <- read.csv("sv-standard.csv")
#df <- read.csv("untrc.csv")
df <- read.csv("./sv-mauricio.csv")
################
# PREPARE DATA #
################
#df <- read.csv("sv-standard.csv")
#df <- read.csv("untrc.csv")
df <- read.csv("./sv-mauricio/sv-mauricio.csv")
# Get list of known first names and last names
knownFirstname <- read_csv("./sv-mauricio/known-firstname.csv")
knownLastname <- read_csv("./sv-mauricio/known-lastname.csv")
################
# PREPARE DATA #
################
#df <- read.csv("sv-standard.csv")
#df <- read.csv("untrc.csv")
df <- read.csv("./sv-mauricio/sv-mauricio.csv")
#df <- df[-c(1,2)]
ent_id <- df$HandID
df <- df[!is.na(ent_id),]
ent_id <- ent_id[!is.na(ent_id)]
df$ID <- 1:nrow(df)
numRecords <- nrow(df)
knownFirstname <- read_csv("./sv-mauricio/known-firstname.csv")
knownFirstname <- knownFirstname$token[knownFirstname$known == 1]
knownLastname <- read_csv("./sv-mauricio/known-lastname.csv")
knownLastname <- knownLastname$token[knownLastname$known == 1]
deptCodes <- read_csv("./code-dept.csv")
deptNeighbors <- read_csv("./neighboring-dept.csv") %>%
inner_join(deptCodes, by = c("DEPT1" = "DEPT")) %>%
inner_join(deptCodes, by = c("DEPT2" = "DEPT")) %>%
transmute(dept1 = as.character(as.numeric(CODE.x)),
dept2 = as.character(as.numeric(CODE.y)), sim = 1.0)
# Add in reverse pairs
deptNeighbors <- bind_rows(deptNeighbors, deptNeighbors %>% rename(dept1 = dept2, dept2 = dept1))
# Get neighboring municipalities
muniCodes <- read_csv("./code-muni.csv")
muniNeighbors <- read_csv("./neighboring-muni.csv") %>%
inner_join(muniCodes, by = c("MUNI1" = "MUNI")) %>%
inner_join(muniCodes, by = c("MUNI2" = "MUNI")) %>%
transmute(muni1 = as.character(as.numeric(CODE.x)),
muni2 = as.character(as.numeric(CODE.y)), sim = 1.0)
# Add in reverse pairs
muniNeighbors <- bind_rows(muniNeighbors, muniNeighbors %>% rename(muni1 = muni2, muni2 = muni1))
# Get neighboring departments
deptCodes <- read_csv("./sv-mauricio/code-dept.csv")
deptNeighbors <- read_csv("./neighboring-dept.csv") %>%
inner_join(deptCodes, by = c("DEPT1" = "DEPT")) %>%
inner_join(deptCodes, by = c("DEPT2" = "DEPT")) %>%
transmute(dept1 = as.character(as.numeric(CODE.x)),
dept2 = as.character(as.numeric(CODE.y)), sim = 1.0)
deptNeighbors <- read_csv("./sv-mauricio/neighboring-dept.csv") %>%
inner_join(deptCodes, by = c("DEPT1" = "DEPT")) %>%
inner_join(deptCodes, by = c("DEPT2" = "DEPT")) %>%
transmute(dept1 = as.character(as.numeric(CODE.x)),
dept2 = as.character(as.numeric(CODE.y)), sim = 1.0)
# Add in reverse pairs
deptNeighbors <- bind_rows(deptNeighbors, deptNeighbors %>% rename(dept1 = dept2, dept2 = dept1))
muniCodes <- read_csv("./sv-mauricio/code-muni.csv")
muniNeighbors <- read_csv("./sv-mauricio/neighboring-muni.csv") %>%
inner_join(muniCodes, by = c("MUNI1" = "MUNI")) %>%
inner_join(muniCodes, by = c("MUNI2" = "MUNI")) %>%
transmute(muni1 = as.character(as.numeric(CODE.x)),
muni2 = as.character(as.numeric(CODE.y)), sim = 1.0)
# Add in reverse pairs
muniNeighbors <- bind_rows(muniNeighbors, muniNeighbors %>% rename(muni1 = muni2, muni2 = muni1))
head(df)
Between 1980 and 1991, the Republic of El Salvador witnessed a civil war between the central government, the left-wing guerrilla Farabundo Marti National Liberation Front (FMLN), and right-wing paramilitary death squads. After the peace agreement in 1992, the United Nations created a Commission on the Truth (UNTC) for El Salvador, which invited members of Salvadoran society to report war-related human rights violations, which mainly focused on killings and disappearances. In order to collect such information the UNTC invited individuals through newspapers, radio, and television advertisements to come forward and testify. The UNTC opened offices through El Salvador where witnesses could provide their testimonials, and this resulted in a list of potential victims with names, date of death, and reported location.
df$firstname[1]
levenshteinSim[2]
levenshteinSim(df$firstname[1], df$firstname[2])
library(RecordLinkage)
library(RecordLinkage)
levenshteinSim(df$firstname[1], df$firstname[2])
unitHispanicSimilarity(df$firstname[1], df$firstname[2])
#' Similarity function for Hispanic names based upon the Monge Elkan metric
#'
#' @param x a character vector
#' @param y a character vector
#' @param sep separator for tokens/words (uses white space by default)
#' @param knownTokens a character vector of known tokens (default is NULL)
#' @returns a length(x) × length(y) similarity matrix
unitHispanicSimilarity <- function(x, y, sep = '\\s+', knownTokens = NULL) {
# Split into tokens (words)
tokens1 <- strsplit(x, sep)
tokens2 <- strsplit(y, sep)
# Preallocate similarity matrix for output
out <- matrix(0.0, nrow = length(tokens1), ncol = length(tokens2))
if (!is.null(knownTokens)) {
# Convert known tokens to environment for faster look-up
knownList <- setNames(replicate(length(knownTokens), 1, simplify = FALSE), knownTokens)
knownEnv <- list2env(knownList, hash = TRUE, size = length(knownList))
}
# Function to compute the symmetrized Monge-Elkan similarity for a single
# pair of tokens
meSim <- function(t1, t2) {
maxSim1 <- numeric(length=length(t1))
knownDistinct1 <- logical(length=length(t1))
maxSim2 <- numeric(length=length(t2))
knownDistinct2 <- logical(length=length(t2))
for (i in seq_along(t1)) {
for (j in seq_along(t2)) {
sim <- unitLevenshteinSimilarity(t1[i], t2[j])
bothKnownDistinct <- FALSE
if (!is.null(knownTokens) && t1[i] != t2[j] &&
exists(t1[i], envir = knownEnv, inherits = FALSE) &&
exists(t2[i], envir = knownEnv, inherits = FALSE)) {
bothKnownDistinct <- TRUE
}
if (sim > maxSim1[i]) { maxSim1[i] <- sim; knownDistinct1[i] <- bothKnownDistinct }
if (sim > maxSim2[j]) { maxSim2[j] <- sim; knownDistinct2[j] <- bothKnownDistinct }
}
}
maxSim1 <- ifelse(knownDistinct1, 0, maxSim1)
maxSim2 <- ifelse(knownDistinct2, 0, maxSim2)
# Symmetrize
return(max(length(t1)/sum(1.0/maxSim1), length(t2)/sum(1.0/maxSim2)))
}
# Function to compute an asymmetric similarity for a single pair of tokens
asymSim <- function(t1, t2) {
if (length(t1) < length(t2)) {
# If t2 contains extra tokens, similarity is zero (can't distort
# true name by adding names)
return(0)
} else {
# Get symmetrized Monge-Elkan similarity
me <- meSim(t1, t2)
# Assign 0.95 weight to Monge-Elkan and 0.05 weight to num. tokens
# similarity
#return(1.0/(0.95/me + 0.05*length(t1)/length(t2)))
return(me)
}
}
# Loop over all combinations in input character vectors
for (i in seq_len(length(tokens1))) {
for (j in seq_len(length(tokens2))) {
out[i, j] <- asymSim(tokens1[[i]], tokens2[[j]])
}
}
return(out)
}
unitHispanicSimilarity(df$firstname[1], df$firstname[2])
unitLevenshteinSimilarity <- function(v1, v2) {
totalLength <- matrix(nchar(v1), nrow=length(v1), ncol=length(v2))
totalLength <- sweep(totalLength, 2, nchar(v2), FUN = "+")
dist <- adist(v1, v2)
ifelse(totalLength > 0, 1.0 - 2.0 * dist / (totalLength + dist) , 1.0)
}
unitHispanicSimilarity(df$firstname[1], df$firstname[2])
#' Similarity function for numeric values
#'
#' @param x a numeric vector
#' @param y a numeric vector
#' @param ran range used to normalize score. Default option of 'NULL' uses
#' the range of c(x, y).
#' @returns a length(x) × length(y) similarity matrix
unitAbsoluteDifference <- function(x, y, ran = NULL) {
x <- as.numeric(x)
y <- as.numeric(y)
if (is.null(ran)) {
ran <- range(c(x, y))
ran <- ran[2] - ran[1]
}
if (!is.numeric.scalar(ran) || ran <= 0) { stop("invalid range") }
out <- matrix(x, nrow=length(x), ncol=length(y))
out <- sweep(out, 2, y, FUN = "-")
out <- 1.0 - abs(out/ran)
if (any(out > 1.0)) warning("scores not on the unit interval")
return(out)
}
x <- 1984
y <- 1981
if (is.null(ran)) {
ran <- range(c(x, y))
ran <- ran[2] - ran[1]
}
ran = NULL
if (is.null(ran)) {
ran <- range(c(x, y))
ran <- ran[2] - ran[1]
}
if (!is.numeric(ran) || ran <= 0) { stop("invalid range") }
out <- matrix(x, nrow=length(x), ncol=length(y))
out
out <- sweep(out, 2, y, FUN = "-")
out
out <- 1.0 - abs(out/ran)
out
